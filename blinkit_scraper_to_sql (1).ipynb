{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a64def6-32a2-476a-b8e8-621272cb853d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://blinkit.com/prn/product/prid/499373\n",
      "https://blinkit.com/prn/product/prid/543\n",
      "https://blinkit.com/prn/product/prid/363754\n",
      "https://blinkit.com/prn/product/prid/442668\n",
      "https://blinkit.com/prn/product/prid/331906\n",
      "https://blinkit.com/prn/product/prid/524797\n",
      "https://blinkit.com/prn/product/prid/555856\n",
      "https://blinkit.com/prn/product/prid/110016\n",
      "https://blinkit.com/prn/product/prid/437645\n",
      "https://blinkit.com/prn/product/prid/403491\n",
      "https://blinkit.com/prn/product/prid/396629\n",
      "https://blinkit.com/prn/product/prid/424615\n",
      "https://blinkit.com/prn/product/prid/556\n",
      "https://blinkit.com/prn/product/prid/11020\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://blinkit.com/prn/product/prid/{}\"\n",
    "product_ids = [\n",
    "    \"499373\", \"543\", \"363754\", \"442668\", \"331906\", \"524797\", \n",
    "    \"555856\", \"110016\", \"437645\", \"403491\", \"396629\", \"424615\", \"556\",\"11020\"\n",
    "]\n",
    "\n",
    "product_urls = []\n",
    "\n",
    "for pid in product_ids:\n",
    "    full_url = base_url.format(pid)\n",
    "    product_urls.append(full_url)\n",
    "\n",
    "# Optional: print the URLs\n",
    "for url in product_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67524ee6-115a-4b04-aae3-3625a30399f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mselenium\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwebdriver\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msupport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m expected_conditions \u001b[38;5;28;01mas\u001b[39;00m EC\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m timedelta\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Initialize WebDriver\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import threading\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "#\"499373\",\"543\",\"363754\",\"442668\",\"331906\",\"524797\",\"555856\",\"110016\",\"437645\",\"403491\",\n",
    "#\"556\",\"11020\"\n",
    "# Example product URLs and zip codes\n",
    "product_ids = [\"499373\",\"543\",\"363754\",\"442668\",\"331906\",\"524797\",\"555856\",\"110016\",\"437645\",\"403491\",\"396629\",\"424615\",\"556\",\"11020\"] \n",
    "base_url = \"https://blinkit.com/prn/product/prid/{}\"\n",
    "zip_codes = [\n",
    "\n",
    "    \"400050\", \"400071\",\"400014\", \"400069\", \"400705\", \"400001\",\n",
    "    # \"411030\", \"411005\", \"411057\", \"411058\", \"411006\", \"410506\",\n",
    "    # \"560001\", \"560038\", \"560068\", \"560076\", \"560100\", \"560043\",\n",
    "    # \"500101\", \"501102\", \"500010\", \"500044\", \"500013\", \"500047\",\n",
    "    # \"110002\", \"110003\", \"110005\", \"110011\", \"110006\", \"110037\"\n",
    "]\n",
    "\n",
    "# Mapping ZIP codes to cities\n",
    "city_mapping = {\n",
    "    \"Mumbai\": [\"400050\", \"400071\", \"400014\", \"400069\", \"400705\", \"400001\"],\n",
    "    \"Pune\": [\"411030\", \"411005\", \"411057\", \"411058\", \"411006\", \"410506\"],\n",
    "    \"Bangalore\": [\"560001\", \"560038\", \"560068\", \"560076\", \"560100\", \"560043\"],\n",
    "    \"Hyderabad\": [\"500101\", \"501102\", \"500010\", \"500044\", \"500013\", \"500047\"],\n",
    "    \"Delhi\": [\"110002\", \"110003\", \"110005\", \"110011\", \"110006\", \"110037\"]\n",
    "}\n",
    "\n",
    "# Assigning location IDs to cities\n",
    "location_mapping = {\n",
    "    \"Pune\": 1,\n",
    "    \"Mumbai\": 2,\n",
    "    \"Bangalore\": 3,\n",
    "    \"Hyderabad\": 4,\n",
    "    \"Delhi\": 5\n",
    "}\n",
    "\n",
    "# Function to get city name and location ID from ZIP code\n",
    "def get_city_and_location(zip_code):\n",
    "    for city, zips in city_mapping.items():\n",
    "        if zip_code in zips:\n",
    "            return city, location_mapping.get(city, 0)  # Default to 0 if not found\n",
    "    return \"Unknown\", 0\n",
    "\n",
    "# Extract brand from product name\n",
    "def extract_brand(product_name):\n",
    "    words = product_name.split()\n",
    "    return words[0] if words else \"Unknown\"\n",
    "\n",
    "# Get current date details\n",
    "scraping_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "year = datetime.date.today().year\n",
    "week_number = datetime.date.today().isocalendar()[1]\n",
    "month = datetime.date.today().month\n",
    "quarter = (month - 1) // 3 + 1\n",
    "\n",
    "# CSV Path\n",
    "csv_file_path = os.path.join(os.getcwd(), 'blinkit_details.csv')\n",
    "\n",
    "# Open CSV file to write the product details\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Crawl_id\",\n",
    "        \"Platform_id\", \"Platform_Name\", \"Location_id\", \"Location_name\", \"Pincode\",\n",
    "        \"Brand_name\", \"Sku_name\",\"Size\",\"Web_pid\",\"Pdp_page_url\",\"Pdp_page_img_url\",\"OSA\", \"OSA_status\", \"Price_RP\",\"Price_SP\",  \n",
    "        \"Discount\", \"Created_on\", \"Year\", \"Week\", \"Month\", \"Quarter\"\n",
    "    ])\n",
    "\n",
    "    for product_id in product_ids:\n",
    "        product_url = base_url.format(product_id)\n",
    "        for zipcode in zip_codes:\n",
    "            try:\n",
    "                # Open product page\n",
    "                driver.get(product_url)\n",
    "                print(f\"Opened product page for {product_url} and zipcode: {zipcode}\")\n",
    "\n",
    "                # Handle any potential popup\n",
    "                try:\n",
    "                    overlay = driver.find_element(By.XPATH, \"//div[contains(@class, 'absolute') and @tabindex='-1']\")\n",
    "                    driver.execute_script(\"arguments[0].style.display = 'none';\", overlay)\n",
    "                    print(\"Popup closed.\")\n",
    "                except:\n",
    "                    print(\"No overlay found.\")\n",
    "\n",
    "                # Click location selector\n",
    "                location_selector = wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[contains(@class, 'LocationBar__DownArrow-sc-x8ezho-5')]\")))\n",
    "                driver.execute_script(\"arguments[0].click();\", location_selector)\n",
    "                print(f\"Clicked location selector for zipcode: {zipcode}\")\n",
    "\n",
    "                #Enter the zipcode\n",
    "                zipcode_input = wait.until(EC.presence_of_element_located((By.XPATH, \"//input[@type='text' and @name='select-locality']\")))\n",
    "                zipcode_input.clear()\n",
    "                zipcode_input.send_keys(zipcode)\n",
    "                print(f\"Entered zipcode: {zipcode}\")\n",
    "                time.sleep(2)\n",
    "\n",
    "                # Try clicking the first location directly\n",
    "                try:\n",
    "                    first_location = WebDriverWait(driver, 3).until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, \"(//div[contains(@class, 'LocationSearchList__LocationDetailContainer-sc-93rfr7-1')])[1]\"))\n",
    "                    )\n",
    "                    driver.execute_script(\"arguments[0].click();\", first_location)\n",
    "                    print(f\"Selected first location for zipcode: {zipcode} (on first try)\")\n",
    "                except:\n",
    "                    print(\"First location not found ‚Äî fallback to alternate entry method.\")\n",
    "\n",
    "                    # Clear the input field using clear() and then re-enter\n",
    "                    zipcode_input.clear()\n",
    "                    print(\"Cleared the zipcode.\")\n",
    "                    time.sleep(2)\n",
    "    \n",
    "                    # Re-enter the same zipcode\n",
    "                    try:\n",
    "                        zipcode_input.send_keys(zipcode)\n",
    "                        print(f\"Re-entered zipcode: {zipcode}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error while re-entering zipcode: {e}\")\n",
    "    \n",
    "    \n",
    "                    # Backspace the last digit\n",
    "                    try:\n",
    "                        zipcode_input = wait.until(EC.presence_of_element_located((By.XPATH, \"//input[@type='text' and @name='select-locality']\")))\n",
    "                        zipcode_input.click()\n",
    "                        zipcode_input.send_keys(Keys.BACKSPACE)\n",
    "                        print(\"Backspaced the last digit.\")\n",
    "                        time.sleep(2)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error while backspacing: {e}\")\n",
    "                        time.sleep(2)\n",
    "                    # Select the first available location\n",
    "                    try:\n",
    "                        first_location = wait.until(EC.element_to_be_clickable((By.XPATH, \"(//div[contains(@class, 'LocationSearchList__LocationDetailContainer-sc-93rfr7-1')])[1]\")))\n",
    "                        driver.execute_script(\"arguments[0].click();\", first_location)\n",
    "                        print(f\"Selected first location for zipcode: {zipcode}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"No locations found for {zipcode}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                # Wait for the page to load\n",
    "                time.sleep(5)\n",
    "                # Extract Product Name\n",
    "                try:\n",
    "                    product_name_element = wait.until(EC.presence_of_element_located((By.XPATH, \"//h1[contains(text(),'Cadbury') or contains(@class, 'tw-text')]\")))\n",
    "                    product_name = product_name_element.text.strip()\n",
    "                except:\n",
    "                    product_name = \"Not Found\"\n",
    "                \n",
    "                brand = extract_brand(product_name)\n",
    "                try:\n",
    "                    price_text = wait.until(EC.presence_of_element_located((By.XPATH, \"(//div[contains(text(), '‚Çπ')])[1]\"))).text.strip()\n",
    "                    prices = [x.strip().replace('‚Çπ', '').replace(',', '') for x in price_text.split(\"MRP\")]\n",
    "                \n",
    "                    # Convert selling price to float or return None if not found\n",
    "                    selling_price = float(prices[0]) if len(prices) > 0 and prices[0].replace('.', '', 1).isdigit() else None\n",
    "                \n",
    "                # Extract only the MRP before % or OFF using regex and convert to float\n",
    "                    if len(prices) > 1:\n",
    "                        combined_value = prices[1].strip()\n",
    "                        print(f\"Combined Value: {combined_value}\")\n",
    "                \n",
    "                        # Case: combined format like \"13010% OFF\"\n",
    "                        if combined_value.endswith('% OFF') and combined_value[:-7].isdigit():\n",
    "                            raw_value = combined_value[:-7]  # Remove '% OFF'\n",
    "                            mrp_price = float(raw_value)  # MRP = all but last 2 digits\n",
    "                                # Last 2 digits = discount\n",
    "                        else:\n",
    "                            # Normal case using regex\n",
    "                            mrp_price_match = re.search(r'\\d+(?=\\s*%|\\s*OFF|$)', combined_value)\n",
    "                            mrp_price = float(mrp_price_match.group()) if mrp_price_match else None\n",
    "                            \n",
    "                    else:\n",
    "                        mrp_price = None\n",
    "                        \n",
    "                \n",
    "                except:\n",
    "                    selling_price = None\n",
    "                    mrp_price = None\n",
    "                \n",
    "                    \n",
    "                # Calculate discount separately\n",
    "                try:\n",
    "                    if selling_price is not None and mrp_price is not None and mrp_price > 0:\n",
    "                        discount =int(((mrp_price - selling_price) / mrp_price) * 100)\n",
    "                        discount = round(discount, 2)\n",
    "                    else:\n",
    "                        discount = None\n",
    "                except:\n",
    "                    discount = None\n",
    "\n",
    "               \n",
    "                if selling_price is None or mrp_price is None:\n",
    "                    OSA = \"Out of Stock\"\n",
    "                    OSA_Status = 0\n",
    "                else:\n",
    "                    OSA = \"In Stock\"\n",
    "                    OSA_Status = 1\n",
    "#size = driver.find_element(By.XPATH,\"//p[contains(@class, 'ProductVariants__VariantUnitText-sc-')]\").text\n",
    "                try:\n",
    "                    size = driver.find_element(\n",
    "                        By.XPATH,\n",
    "                        \"//div[p/span[text()='Unit']]/div\"\n",
    "                    ).text\n",
    "                except:\n",
    "                    size = \"N/A\"\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # elements = driver.find_elements(By.XPATH, \"//div[contains(@class, 'ProductAttribute__ProductAttributesDescription-sc-') and contains(@class, 'jJVAqC')]\")\n",
    "                # for el in elements:\n",
    "                #     print(el.text)\n",
    "\n",
    "       \n",
    "                try:\n",
    "                    image_url = driver.find_element(By.XPATH, \"//div[contains(@class, 'ProductCarousel__ImageContainer-sc-')]//img\").get_attribute(\"src\")\n",
    "                except:\n",
    "                    image_url = \"N/A\"\n",
    "\n",
    "                # Get City and Location ID\n",
    "                city_name, location_id = get_city_and_location(zipcode)\n",
    "\n",
    "                # Write to CSV\n",
    "                writer.writerow([2100,2, \"Blinkit\", location_id, city_name, zipcode,\n",
    "                                  brand, product_name, size,product_id,product_url,\n",
    "                                  image_url,OSA, OSA_Status, mrp_price, selling_price,discount, scraping_date,\n",
    "                                  year, week_number, month, quarter])\n",
    "\n",
    "                print(f\"Data written for zipcode {zipcode}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing ZIP {zipcode} for {product_url}: {e}\")\n",
    "\n",
    "# Close WebDriver\n",
    "driver.quit()\n",
    "print(f\"CSV file has been saved at: {csv_file_path}\")\n",
    "\n",
    "\n",
    "df=pd.read_csv(r\"C:\\Users\\SHIVANI\\Desktop\\Web_scrapping\\blinkit_details.csv\")\n",
    "df = df.replace({pd.NA: None, pd.NaT: None, float('nan'): None, 'nan': None})\n",
    "\n",
    "\n",
    "import pymysql\n",
    "from datetime import datetime\n",
    " \n",
    "# Replace with your cloud database details\n",
    "CLOUD_DB_CONFIG = {\n",
    "    \"host\": \"database-1.cq5s62s644gu.us-east-1.rds.amazonaws.com\",  # Example: 'your-db-instance.us-east-1.rds.amazonaws.com'\n",
    "    \"database\": \"cadbury\",\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"JPT_DA12345\",\n",
    "    \"port\": 3306  # Default MySQL port\n",
    "}\n",
    " \n",
    "# Connect to MySQL\n",
    "conn = pymysql.connect(**CLOUD_DB_CONFIG)\n",
    "cursor = conn.cursor()\n",
    " \n",
    "# Create Master Table (Permanent Storage)\n",
    "# cursor.execute('''\n",
    "#     CREATE TABLE IF NOT EXISTS cadbury_master (\n",
    "#         id SERIAL PRIMARY KEY,\n",
    "#         Crawl_id INT,  \n",
    "#         Platform_id INT,\n",
    "#         Platform_Name VARCHAR(8),\n",
    "#         Location_id INT,\n",
    "#         Location_name VARCHAR(9),\n",
    "#         Pincode INT,\n",
    "#         Brand_name VARCHAR(7),\n",
    "#         Sku_name VARCHAR(54),\n",
    "#         Size VARCHAR(4),\n",
    "#         Web_pid VARCHAR(16),\n",
    "#         Pdp_page_url VARCHAR(60),\n",
    "#         Pdp_page_img_url VARCHAR(255),\n",
    "#         OSA VARCHAR(12),\n",
    "#         OSA_status INT,\n",
    "#         Price_RP INT,\n",
    "#         Price_SP INT,\n",
    "#         Discount INT,\n",
    "#         Created_on TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "#         Year INT,\n",
    "#         Week INT,\n",
    "#         Month INT,\n",
    "#         Quarter INT\n",
    "#     )\n",
    "# ''')\n",
    " \n",
    "# Create Persistent Temporary Table (Crawl blinkit)\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS crawl_blinkit (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        Crawl_id INT DEFAULT 1700,  \n",
    "        Platform_id INT,\n",
    "        Platform_Name VARCHAR(8),\n",
    "        Location_id INT,\n",
    "        Location_name VARCHAR(9),\n",
    "        Pincode INT,\n",
    "        Brand_name VARCHAR(7),\n",
    "        Sku_name VARCHAR(54),\n",
    "        Size VARCHAR(4),\n",
    "        Web_pid VARCHAR(16),\n",
    "        Pdp_page_url VARCHAR(60),\n",
    "        Pdp_page_img_url VARCHAR(255),\n",
    "        OSA VARCHAR(12),\n",
    "        OSA_status INT,\n",
    "        Price_RP INT,\n",
    "        Price_SP INT,\n",
    "        Discount INT,\n",
    "        Created_on TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        Year INT,\n",
    "        Week INT,\n",
    "        Month INT,\n",
    "        Quarter INT\n",
    "    )\n",
    "''')\n",
    " \n",
    "# # Create Job Tracker Table (Stores Last Run Time)\n",
    "# cursor.execute('''\n",
    "#     CREATE TABLE IF NOT EXISTS job_tracker (\n",
    "#         id SERIAL PRIMARY KEY,\n",
    "#         last_run TIMESTAMP\n",
    "#     )\n",
    "# ''')\n",
    " \n",
    "# Insert New Data into Crawl Amazon (Example Data)\n",
    " \n",
    "# Insert New Data into Crawl blinkit (Example DataFrame: df)\n",
    "for row in df.itertuples(index=False):\n",
    "    cursor.execute('''\n",
    "        INSERT INTO crawl_blinkit (\n",
    "            Crawl_id, Platform_id, Platform_Name, Location_id, Location_name, Pincode, Brand_name,\n",
    "            Sku_name, Size, Web_pid, Pdp_page_url, Pdp_page_img_url, OSA, OSA_status,\n",
    "            Price_RP, Price_SP, Discount, Created_on, Year, Week, Month, Quarter\n",
    "        )\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    ''', (\n",
    "        row.Crawl_id, row.Platform_id, row.Platform_Name, row.Location_id, row.Location_name,\n",
    "        row.Pincode, row.Brand_name, row.Sku_name, row.Size, row.Web_pid,\n",
    "        row.Pdp_page_url, row.Pdp_page_img_url, row.OSA, row.OSA_status,\n",
    "        row.Price_RP, row.Price_SP, row.Discount, row.Created_on,\n",
    "        row.Year, row.Week, row.Month, row.Quarter\n",
    "    ))\n",
    "conn.commit()\n",
    "print(\"‚úÖ New data inserted into crawl_blinkit table.\")\n",
    " \n",
    "# # Check Last Execution Time\n",
    "# cursor.execute(\"SELECT last_run FROM job_tracker ORDER BY id DESC LIMIT 1\")\n",
    "# last_run = cursor.fetchone()\n",
    " \n",
    "# current_time = datetime.now()\n",
    " \n",
    "# # If no record exists, insert the first run time\n",
    "# if last_run is None:\n",
    "#     print(\"üïí First time running the script. Setting initial timestamp.\")\n",
    "#     cursor.execute(\"INSERT INTO job_tracker (last_run) VALUES (%s)\", (current_time,))\n",
    "#     conn.commit()\n",
    "# else:\n",
    "#     last_run_time = last_run[0]\n",
    "#     time_difference = current_time - last_run_time\n",
    " \n",
    "#     # Move Data to Master Table if 12 Hours Passed\n",
    "#     if time_difference >= timedelta(hours=12):\n",
    "#         print(f\"üîÑ Moving data at {current_time} (Last run: {last_run_time})...\")\n",
    " \n",
    "#         cursor.execute('''\n",
    "#             INSERT INTO master_table (crawl_id, platform_id, Platform_Name, Location_id, Location_name, Pincode, Brand_name,\n",
    "#             Sku_name, Size, Web_pid, Pdp_page_url, image_url, OSA, OSA_Status, Price_RP, Price_SP, Discount, Created_on, Year, Week, Month, Quarter)\n",
    "#             SELECT crawl_id, platform_id, Platform_Name, Location_id, Location_name, Pincode, Brand_name,\n",
    "#             Sku_name, Size, Web_pid, Pdp_page_url, image_url, OSA, OSA_Status, Price_RP, Price_SP, Discount, Created_on, Year, Week, Month, Quarter\n",
    "#             FROM crawl_amazon\n",
    "#         ''')\n",
    " \n",
    "#         cursor.execute(\"DELETE FROM crawl_blinkit\")  # Clears only moved data, keeping table intact\n",
    "#         cursor.execute(\"UPDATE job_tracker SET last_run = %s WHERE id = (SELECT MAX(id) FROM job_tracker)\", (current_time,))\n",
    "#         conn.commit()\n",
    " \n",
    "#         print(\"‚úÖ Data moved to master_table and timestamp updated.\")\n",
    "#     else:\n",
    "#         print(f\"‚è≥ Less than 12 hours since last run ({time_difference}). Skipping move.\")\n",
    " \n",
    "# Close Connection\n",
    "conn.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
